<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models</title>

    <!-- 引入 Bootstrap 和自定义样式 -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="style.css"> <!-- 你可以自己定义 style.css -->
</head>
<body>


<div class="container text-center" style="margin-top: 40px;">
    <h1>Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models</h1>
    <div class="authors">
        <p>
            Chaolei Han<sup>1</sup>, Hongsong Wang<sup>1</sup>, Jidong Kuang<sup>1</sup>, Lei Zhang<sup>2</sup>, Jie Gui<sup>1</sup> <br>
            <sup>1</sup>Southeast University &nbsp; <sup>2</sup>Nanjing Normal University
        </p>
        <p>
            <a href="https://arxiv.org/abs/2501.13795" target="_blank">[Paper]</a>
            <a href="https://github.com/Chaolei98/FreeZAD" target="_blank">[Code]</a>
        </p>
    </div>
</div>


<div class="container">
    <h2>Abstract</h2>
    <p>
        Existing zero-shot temporal action detection (ZSTAD) methods predominantly use fully supervised or unsupervised strategies to recognize unseen activities. 
        However, these training-based methods are susceptible to domain shifts and entail high computational costs. 
        Unlike previous works, we propose a training-Free Zero-shot temporal Action Detection (FreeZAD) method, leveraging image-pretrained vision-language models (VLMs) to directly classify and localize unseen activities within untrimmed videos.
        We mitigate the need for explicit temporal modeling and reliance on pseudo-label quality by designing the Logarithmic decay weighted Outer-Inner-Contrastive Score (LogOIC) and frequency-based actionness calibration.
        Furthermore, we introduce a test-time adaptation (TTA) strategy using Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling VLMs to adapt more effectively for ZSTAD.
        Extensive experiments on the THUMOS14 and ActivityNet-1.3 datasets demonstrate that our training-free method outperforms state-of-the-art unsupervised methods while requiring only 1/13 of the runtime. 
        When equipped with TTA, the enhanced method further narrows the gap with fully supervised training methods of ZSTAD.
    </p>
</div>

    
<div class="container">
    <h2>Samples</h2>
    <div style="text-align: center;">
        <img src="visualization.png" alt="Teaser Image" class="img-responsive" style="display:inline-block; max-width:80%;">
    </div>
    <p>
        We visualize the detection results of our AdaZAD method and a previous unsupervised SOTA method on three videos from the THUMOS14 dataset. 
        These videos represent samples of varying difficulty: easy, moderate, and hard cases.
        Each frame is uniformly sampled from the respective videos.
        In sample (a), the action `Long Jump' is accurately detected due to its clear distinction from the background, where the subject is simply standing still.
        In sample (b), our detection starts with the `clean' activity and ends after the `jerk', as indicated by its name. We believe the failure to detect the preparatory action is due to the lack of adequate descriptive information in the action name.
        Sample (c) illustrates an instance of failed detection, where the model confuses the distinction between the swing movement and a person holding a tennis racket who is not actively swinging. 
        Despite certain limitations, our results demonstrate greater reliability compared to those produced by the state-of-the-art unsupervised method.
</div>
